---
title: "Titanic: Machine Learning from Disaster"
author: "Abhijeet Sharma"
date: "Wednesday, July 08, 2015"
output: html_document
---
```
The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.

One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.

In this challenge, we analyse what sorts of people were likely to survive. In particular, we apply the tools of machine learning to predict which passengers survived the tragedy.
```
# Step 1: Fetching data from the training and testing files
```{r}
inp_train <- read.csv(file="../data/train.csv", header=TRUE, sep=",", quote="\"", 
                  stringsAsFactors=FALSE, na.strings=c("NA",""))
inp_test <- read.csv(file="../data/test.csv", header=TRUE, sep=",", quote="\"", 
                  stringsAsFactors=FALSE, na.strings=c("NA",""))
```

We now check the structure of the training dataset
```{r}
str(inp_train)
```
We observe that there are many fields we can convert to factors.

# Step 2: Pre-processing.
## Step 2.1: Cleaning the training dataset.
```{r}
# Passenger Id has no meaning for this analysis, so we remove it from the 
# training dataset.
train <-  subset(inp_train, select= -c(PassengerId))
# There are some columns which are more meaningful if we convert them to factors.
train$Survived <- factor(train$Survived, levels = c("0", "1"), 
                         labels = c("Not_Survived", "Survived"))
train$Pclass <- factor(train$Pclass)
train$Sex <- factor(train$Sex)
train$Embarked <- factor(train$Embarked)
```
Now that all the proper columns are converted to factors, we move on to handling
the "NA" fields.
## Step 2.2: Feature Engineering 
```{r}
# This time, instead of discarding name, we try to find some use of it.
## One way is to extract the Titles from the name and see if there is any link to
# survivality (in the Titanic movie, priest(title Rev.) were more likely to stay in the 
# sinking ship)
## Another way is to use the family name
first_name <- gsub("[A-Za-z '-]*, ", "", train$Name)
Title <- gsub("[. *] .*","",first_name)
# View(data.frame(train$Name, gsub("[A-Za-z '-]*, ", "", train$Name), gsub("[. *] .*","",gsub("[A-Za-z '-]*, ", "", train$Name))))

Title[Title=="Col" | Title=="Capt" | Title=="Major"] <- "Army"
Title[Title=="Sir"| Title=="Don" | Title=="Jonkheer" | Title=="Dona" | 
          Title=="the Countess" | Title=="Lady"] <- "Royal"
Title[Title=="Miss" | Title=="Mlle" | Title=="Ms"] <- "Non-royal Unmarried Female"
Title[Title=="Mrs" | Title=="Mme"] <- "Non-royal Married Female"

train$Title <- as.factor(Title)
prop.table(table(train$Title, train$Sex, train$Survived), 1)
```
## Step 2.3: Handling NA's

This time, we'll create a version of our train set which handles NAs in Age and 
Embarked columns and observe the performance.
```{r}
# We ignore the "Name", "Cabin" and "Ticket" columns.
# TODO: analyze any use of the "Cabin" and "Ticket" columns.
train <- subset(train, select= -c(Cabin, Name, Ticket))
summary(train)
summary(inp_test)
```
In the Train set, We observe that both Age(177) and Embarked(2) columns have NAs.
Also, the Fare column has 15 zero fares.

In the Test set, we observe that Age(86) and Fare(1) columns have NAs.
Also, the Fare column has 2 zero fares.

We'll use regression to predict the NAs in the Age column, 
We'll not consider the 2 Embarked rows which have NAs now.
We'll ignore the zero fares for now.
```{r message=FALSE}
na_Embarked <- which(is.na(train$Embarked))
train <- train[-na_Embarked,]

na_Age <- which(is.na(train$Age))
train.woNA <- train[-na_Age,]
train.na <- train[na_Age,]

library(caret)
set.seed(123)
fit1 <- lm(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title, 
           data=train.woNA)
predFit1 <- predict(fit1, train.woNA[,-c(1, 4)])

set.seed(123)
fit2 <- lm(Age ~ Pclass + Sex + SibSp + Fare + Embarked + Title, data=train.woNA)
predFit2 <- predict(fit2, subset(train.woNA, select=c(Pclass, Sex, SibSp, Fare, 
                                                      Embarked, Title)))

set.seed(123)
fit3 <- lm(Age ~ Pclass + Sex + SibSp + Fare + Title, data=train.woNA)
predFit3 <- predict(fit3, subset(train.woNA, select=c(Pclass, Sex, SibSp, Fare, 
                                                      Title)))

set.seed(123)
fit4 <- lm(Age ~ Pclass + Sex + SibSp + Title, data=train.woNA)
predFit4 <- predict(fit4, subset(train.woNA, select=c(Pclass, Sex, SibSp, Title)))

set.seed(123)
# Using step function
step(fit1, ~.^2)
set.seed(123)
fit5 <- lm(formula = Age ~ Pclass + SibSp + Parch + Fare + Embarked + Title +
               Parch:Title + SibSp:Parch, data = train.woNA)
predFit5 <- predict(fit5, train.woNA)
anova(fit1, fit2, fit3, fit4, fit5)

par(mfrow=c(2,2))
plot(fit5)
par(mfrow=c(1,1))

set.seed(123)
fit6 <- train(Age ~., data=train.woNA[,-1], method="rf")
predFit6 <- predict(fit6, train.woNA)

set.seed(123)
fit7 <- rpart(Age ~ ., data=train.woNA)
predFit7 <- predict(fit7, train.woNA)
fancyRpartPlot(fit7)

df <- data.frame(predFit1, predFit2, predFit3, predFit4, predFit5, predFit6, 
                 predFit7, train.woNA$Age)
# fit 5 and fit 6 look closest to the actual age, so lets create a ensemble 
# model combining both.

age_Ensemble_df <- data.frame(pStep=predFit5, pForest=predFit6, Age=train.woNA$Age)
set.seed(123)
fit8 <- train(Age ~., method="gbm", data=age_Ensemble_df)
predFit8 <- predict(fit8, age_Ensemble_df[,-3])

df <- data.frame(predFit5, predFit6, predFit8, train.woNA$Age)
#View(df)

# Selecting fit8 as our final model
na_age_pred5 <- predict(fit5, train.na[,-4])
na_age_pred6 <- predict(fit6, train.na[,-4])
na_age_Ensemble_df <- data.frame(pStep=na_age_pred5, pForest=na_age_pred6)
age_predictions <- predict(fit8, na_age_Ensemble_df)
# sum(age_predictions < 0)
train$Age[na_Age] <- age_predictions
```

# Step 3: Analyze Plots
## Survivality
```{r echo = FALSE, message=FALSE}
library(ggplot2)
library(gridExtra)

# histogram for Survived column.
ggplot(data=train, aes(Survived)) + geom_histogram(binwidth=0.5) + 
    ggtitle("Fig.1: Histogram of Survivality")

table(train$Survived)
prop.table(table(train$Survived))
```
We observe from this plot that almost 60% didn't survive the disaster.
## Survivality by Age and Sex
```{r echo = FALSE, message=FALSE}
# Jitter plot for Age, Sex and Survived columns.
ggplot(data=train, aes(x=Age, y=Sex, color=Survived)) + 
    geom_point(position="jitter") + 
    ggtitle("Fig.2: Comparision of Survivality by Age and Sex") + 
    xlab("Age (in years)")
table(train$Sex, train$Survived)
prop.table(table(train$Sex, train$Survived),1)
```

We observe from this plot that higher percentage of females across all ages
survived whereas for males, there was higher survival rate for children but 
lower survival rate for adults.This confirms the "Women and Children First" 
policy used for the lifeboats.
## Survivality by no. of Siblings/Spouse and Age
```{r echo=FALSE}
ggplot(data=train, aes(x=SibSp, y=Age, color=Survived)) + 
    geom_point(position="jitter") + 
    ggtitle("Fig.3: Comparision of Survivality by Age and Sex") +
    xlab("No. of Siblings") + ylab("Age (in years)")
```

We observe from the above plot that as the no. of siblings/spouse increase for ages<20,
survivality has decreased.
We plot a histogram to confirm the above observation.
```{r echo=FALSE, message=FALSE}
library(scales)
ggplot(data=train, aes(x=SibSp, fill=Survived)) + 
    geom_bar(aes(y=(..count..)/sum(..count..))) + 
    scale_y_continuous(labels=percent) + 
    ggtitle("Fig.4: Percentage plot of no.of Siblings/Spouse and Survivality") + 
    xlab("Percentage of counts") + ylab("No. of Siblings")
```

The distribution of the number of siblings/spouse is not uniform to prove our theory.
we plot another histogram and this time we show the relative frequencies.
```{r echo=FALSE, message=FALSE}
ggplot(train, aes(x=SibSp, fill=Survived)) + geom_bar(position="fill") + 
    ggtitle("Fig.5: Relative Frequencies of no.of Siblings/Spouse and Survivality") + 
    xlab("No. of Siblings") 
```

The above plot confirms that as the no. of siblings/spouse increase for ages <20, the 
survivality chances decreases.
We also observe that the survival rate of people with zero siblings/spouse is around 10% 
lower than the survival rate of people with 1 sibling. 
From Fig.3, we can infer that this is because most people with zero siblings/spouse are
ages>20 who (from Fig.2) had lower survivality rate.

## Survivality by Age Group
```{r}
train$AgeGroup <- cut(train$Age, breaks = c(0, 8, 13, 18, 60, Inf), labels = c("Child", "Teenager", "Young Adult", "Adult", "Elder"), right=FALSE)

prop.table(table(train$AgeGroup, train$Survived),1)

ggplot(data=train, aes(AgeGroup, fill=Survived)) + geom_histogram(binwidth=1)
```

# Step 4: Create Data Partitions
We divide the training set into 3 parts, in a 60:20:20 manner.
```{r message=FALSE}
# TODO: Use different splitting methods: k-fold, repeat k-fold, leave-one out.
library(caret)
train <- subset(train, select=-AgeGroup)
set.seed(123) # setting seed for reproducibility
# We first divide the training set into two parts in 60:40 ratio.
inTrain <- createDataPartition(y=train$Survived, p = 0.6, list=FALSE)
training.train <- train[inTrain,]
rest_train <- train[-inTrain,]

set.seed(123) # setting seed for reproducibility
# We now divide the rest of the training set into two parts in 50:50 ratio.
# This splits the original training set in a 60:20:20 ratio.
inTest <- createDataPartition(y=rest_train$Survived, p = 0.5, list=FALSE)
# 20% data for validation set to select best classifier
training.classifier_valid <- rest_train[inTest,] 
# 20% data for validation set to calculate out-of-sample errors
training.oos_valid <- rest_train[-inTest,] 

# data_labels <- c("Train", "Validate", "Test")
# per_data_point_label <- sample(data_labels, size=nrow(train), 
#                                replace=TRUE, prob=c(0.6, 0.2, 0.2))
# training.train <- train[per_data_point_label == "Train",]
# training.classifier_valid <- train[per_data_point_label == "Validate",]
# training.oos_valid <- train[per_data_point_label == "Test",]
```
# Step 5: Select the best classifier
We select among different classifiers (Decision trees, Random Forests, 
Naive Bayes, etc.) by training on training.train set and and predicting on 
training.classifier_valid.
```{r message=FALSE,warning=FALSE}
# TODO: Use interactions b/w variables
library(klaR)
par(mfrow=c(1,1))
# Decision Trees
set.seed(123)
tree_Model <- train(Survived ~ ., data=training.train, method="rpart")
pred_Tree <- predict(tree_Model, newdata=training.classifier_valid[,-1])
conf_Tree <- confusionMatrix(pred_Tree, training.classifier_valid$Survived)
# Decision Tree Plot
library(rattle)
fancyRpartPlot(tree_Model$finalModel)

# Random Forest
set.seed(123)
forest_Model <- train(Survived ~ ., data=training.train, method="rf")
pred_Forest <- predict(forest_Model, newdata=training.classifier_valid[,-1])
conf_Forest <- confusionMatrix(pred_Forest, training.classifier_valid$Survived)

# Naive Bayes
set.seed(123)
naiveB_Model <- NaiveBayes(Survived ~ ., data=training.train)
pred_naiveB <- predict(naiveB_Model, newdata=training.classifier_valid[,-1])
conf_naiveB <- confusionMatrix(pred_naiveB$class, training.classifier_valid$Survived)

#pred1 <- predict(mod1, testing)
#pred2 <- predict(logit_Model, testing)
#qplot(pred1, pred2, color=wage, data=testing)

# Ensemble: Fit a model that combines predictors
df_Ensemble <- data.frame(pTree=pred_Tree, pForest=pred_Forest, 
                          pNaive=pred_naiveB$class, 
                          Survived=training.classifier_valid$Survived)
set.seed(123)
ensemble_Model <- train(Survived ~., method="gbm", data=df_Ensemble)
pred_Ensemble <- predict(ensemble_Model, df_Ensemble[,-4])
conf_Ensemble <- confusionMatrix(pred_Ensemble, df_Ensemble$Survived)

# Boost
set.seed(123)
boost_Model <- train(Survived ~., method="gbm", data=training.train)
pred_Boost <- predict(boost_Model, newdata=training.classifier_valid[,-1])
conf_Boost <- confusionMatrix(pred_Boost, training.classifier_valid$Survived)
```
Now that all the predictions are complete, we compare them
```{r}
# Comparison of predictions from each model
compare_df <- data.frame(Accuracy = c(conf_Tree$overall[1], 
                                      conf_Forest$overall[1], 
                                      conf_naiveB$overall[1],
                                      conf_Ensemble$overall[1],
                                      conf_Boost$overall[1]),
                         row.names = c("rpart", "rf", "NaiveB", "Ensemble", "Boost"))
compare_df
```
We observe that both **Random Forest** and **Ensemble** gives us the highest Accuracy,
we see how the above model fares with the training.oos_valid dataset
```{r}
pred_Tree_oos <- predict(tree_Model, newdata=training.oos_valid[,-1])
conf_Tree_oos <- confusionMatrix(pred_Tree_oos, training.oos_valid$Survived)

pred_Forest_oos <- predict(forest_Model, newdata=training.oos_valid[,-1])
conf_Forest_oos <- confusionMatrix(pred_Forest_oos, training.oos_valid$Survived)

pred_naiveB_oos <- predict(naiveB_Model, newdata=training.oos_valid[,-1])
conf_naiveB_oos <- confusionMatrix(pred_naiveB_oos$class, training.oos_valid$Survived)

df_ensemble_oos <- data.frame(pTree=pred_Tree_oos, pForest=pred_Forest_oos, 
                              pNaive=pred_naiveB_oos$class, 
                              Survived=training.oos_valid$Survived)
pred_Ensemble_oos <- predict(ensemble_Model, df_ensemble_oos[,-4])
conf_Ensemble_oos <- confusionMatrix(pred_Ensemble_oos, df_ensemble_oos$Survived)

pred_Boost_oos <- predict(boost_Model, newdata=training.oos_valid[,-1])
conf_Boost_oos <- confusionMatrix(pred_Boost_oos, training.oos_valid$Survived)

compare_df_oos <- data.frame(Accuracy = c(conf_Tree_oos$overall[1], 
                                          conf_Forest_oos$overall[1], 
                                          conf_naiveB_oos$overall[1],
                                          conf_Ensemble_oos$overall[1],
                                          conf_Boost_oos$overall[1]),
                             row.names = c("rpart", "rf", "NaiveB", "Ensemble", "Boost"))
compare_df_oos
```
We observe that again **Random Forest** and **Ensemble** gives us the highest 
out-of-sample accuracy.
We select the Rnadom Forest classifier for predicting the test set.
# Step 6: Predict on Test set
## Step 6.1: Clean Test set and syncc columns with training set
```{r}
# Feature Engineering
test <- inp_test
first_name <- gsub("[A-Za-z '-]*, ", "", test$Name)
Title <- gsub("[. *] .*","",first_name)
# View(data.frame(test$Name, gsub("[A-Za-z '-]*, ", "", test$Name), gsub("[. *] .*","",gsub("[A-Za-z '-]*, ", "", test$Name))))

Title[Title=="Col" | Title=="Capt" | Title=="Major"] <- "Army"
Title[Title=="Sir"| Title=="Don" | Title=="Jonkheer" | Title=="Dona" | 
          Title=="the Countess" | Title=="Lady"] <- "Royal"
Title[Title=="Miss" | Title=="Mlle" | Title=="Ms"] <- "Non-royal Unmarried Female"
Title[Title=="Mrs" | Title=="Mme"] <- "Non-royal Married Female"

test$Title <- as.factor(Title)
prop.table(table(test$Title, test$Sex), 1)

# remove unnecesary columns
test <- subset(test, select = -c(Name, Ticket, Cabin))
test$Pclass <- factor(test$Pclass)
test$Sex <- factor(test$Sex)
test$Embarked <- factor(test$Embarked)
test$Fare[which(is.na(test$Fare))] <- 0
na_Age <- which(is.na(test$Age))
test.na <- test[na_Age,]
# Predicting the age in the test set using the fit8 model
test_na_age_pred5 <- predict(fit5, test.na[,-c(1,4)])
test_na_age_pred6 <- predict(fit6, test.na[,-c(1,4)])
test_na_age_Ensemble_df <- data.frame(pStep=test_na_age_pred5, 
                                      pForest=test_na_age_pred6)
test_age_predictions <- predict(fit8, test_na_age_Ensemble_df)
# sum(test_age_predictions < 0) # only fit6 has sum = 0
test$Age[na_Age] <- test_age_predictions
```
## Step 6.2: predict on the classifier selected above
```{r}
# we train the selected classifier using the full training set and predict on
# the test set
set.seed(123)
forest_Model <- train(Survived ~ ., data=train, method="rf")
pred_Forest_test <- predict(forest_Model, newdata=test)
varImp(forest_Model)
importance(forest_Model$finalModel)
# varImpPlot(my_forest)

# converting factor to numeric(Not_Survived=0, Survived=1)
final_pred <- as.numeric(pred_Forest_test=="Survived")
df <- data.frame(PassengerId=test$PassengerId, Survived=final_pred)    
```
# Step 7: Write results to csv and upload to Kaggle
```{r}
write.csv(df, file="prediction_rf.csv", row.names=FALSE)
```
Accuracy: 0.79904.
Hurray!
We find that the above model results in an accuracy of 0.79904 in Kaggle.
That's Awesome! and far better than our earlier score of 0.78469.
So, Using the Title as a feature paid off:)
Again, just for fun, let's predict on the test set using some of the other 
classfiers.
```{r}
# Decision Tree
set.seed(123)
tree_Model <- train(Survived ~ ., data=train, method="rpart")
pred_Tree_test <- predict(tree_Model, newdata=test)
fancyRpartPlot(tree_Model$finalModel)

# converting factor to numeric(Not_Survived=0, Survived=1)
final_pred <- as.numeric(pred_Tree_test=="Survived")
df <- data.frame(PassengerId=test$PassengerId, Survived=final_pred)

write.csv(df, file="prediction_tree.csv", row.names=FALSE)
```
Decision Tree scored 0.78947, better than 0.78469 but less than the Rf Model.

```{r}
# Modified Decision Tree
cont <- rpart.control(minsplit=50, cp=0)
my_tree_three <- rpart(formula=Survived ~ ., data=train, method="class", control=cont)
pred_tree_test <- predict(my_tree_three, newdata=test, type="class")
fancyRpartPlot(my_tree_three)

final_pred <- as.numeric(pred_tree_test=="Survived")
df <- data.frame(PassengerId=test$PassengerId, Survived=final_pred)
write.csv(df, file="prediction_tree_control.csv", row.names=FALSE)
```
Modified Decision Tree Accuracy is 0.77033

```{r results='hide'}
# Naive Bayes
set.seed(123)
naiveB_Model <- NaiveBayes(Survived ~ ., data=train)
pred_naiveB_test <- predict(naiveB_Model, newdata=test)

# converting factor to numeric(Not_Survived=0, Survived=1)
final_pred <- as.numeric(pred_naiveB_test$class=="Survived")
df <- data.frame(PassengerId=test$PassengerId, Survived=final_pred)

write.csv(df, file="prediction_naiveB.csv", row.names=FALSE)
```
Naive Bayes scored 0.75120

```{r}
# Ensemble
df_Ensemble_test <- data.frame(pTree=pred_Tree_test, pForest=pred_Forest_test, 
                               pNaive=pred_naiveB_test$class)
pred_Ensemble_test <- predict(ensemble_Model, newdata=df_Ensemble_test)

# converting factor to numeric(Not_Survived=0, Survived=1)
final_pred <- as.numeric(pred_Ensemble_test=="Survived")
df <- data.frame(PassengerId=test$PassengerId, Survived=final_pred)

write.csv(df, file="prediction_ensemble.csv", row.names=FALSE)
```
Ensemble scored 0.75120

```{r}
# Generalized Boosted Regression Models
set.seed(123)
boost_Model <- train(Survived ~., method="gbm", data=train)
pred_Boost_test <- predict(boost_Model, newdata=test)

# converting factor to numeric(Not_Survived=0, Survived=1)
final_pred <- as.numeric(pred_Boost_test=="Survived")
df <- data.frame(PassengerId=test$PassengerId, Survived=final_pred)

write.csv(df, file="prediction_boost.csv", row.names=FALSE)
```
Boost scored 0.76077

```
We observe from the above values that our Random Forest model was the best model.
But I believe it can be made even better.

```
Next, we decide on ways to use the Ticket and Cabin columns and see if we 
can improve the score.
