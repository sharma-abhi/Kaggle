---
title: "Titanic: Machine Learning from Disaster"
author: "Abhijeet Sharma"
date: "Monday, July 06, 2015"
output: html_document
---
```
The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.

One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.

In this challenge, we analyse what sorts of people were likely to survive. In particular, we apply the tools of machine learning to predict which passengers survived the tragedy.
```
# Step 1: Fetching data from the training and testing files
```{r}
inp_train <- read.csv(file="../data/train.csv", header=TRUE, sep=",", quote="\"", 
                  stringsAsFactors=FALSE, na.strings=c("NA",""))
inp_test <- read.csv(file="../data/test.csv", header=TRUE, sep=",", quote="\"", 
                  stringsAsFactors=FALSE, na.strings=c("NA",""))
```

We now check the structure of the training dataset
```{r}
str(inp_train)
```
We observe that there are many fields we can convert to factors.

# Step 2: Pre-processing.
## Step 2.1: Cleaning the training dataset.
```{r}
# Passenger Id has no meaning for this analysis, so we remove it from the 
# training dataset.
train <-  subset(inp_train, select= -c(PassengerId))
# There are some columns which are more meaningful if we convert them to factors.
train$Survived <- factor(train$Survived, levels = c("0", "1"), 
                         labels = c("Not_Survived", "Survived"))
train$Pclass <- factor(train$Pclass)
train$Sex <- factor(train$Sex)
train$Embarked <- factor(train$Embarked)
```
Now that all the proper columns are converted to factors, we move on to handling
the "NA" fields.

## Step 2.2: Handling NA's

This time, we'll create a version of our train set which handles NAs in Age and 
Embarked columns and observe the performance.
```{r}
# We are still ignoring the "Name", "Cabin" and "Ticket" columns for now though.
# TODO: analyze any use of the "Name", "Cabin" and "Ticket" columns.
train <- subset(train, select= -c(Cabin, Name, Ticket))
summary(train)
summary(inp_test)
```
In the Train set, We observe that both Age(177) and Embarked(2) columns have NAs.
Also, the Fare column has 15 zero fares.

In the Test set, we observe that Age(86) and Fare(1) columns have NAs.
Also, the Fare column has 2 zero fares.

We'll use regression to predict the NAs in the Age column, 
We'll not consider the 2 Embarked rows which have NAs now.
We'll ignore the zero fares for now.
```{r message=FALSE}
na_Embarked <- which(is.na(train$Embarked))
train <- train[-na_Embarked,]

na_Age <- which(is.na(train$Age))
train.woNA <- train[-na_Age,]
train.na <- train[na_Age,]

library(caret)
set.seed(123)
fit1 <- lm(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked, data=train.woNA)
predFit1 <- predict(fit1, train.woNA[,-c(1, 4)])

set.seed(123)
fit2 <- lm(Age ~ Pclass + Sex + SibSp + Fare + Embarked, data=train.woNA)
predFit2 <- predict(fit2, train.woNA[,c(2, 3, 5, 7, 8)])

set.seed(123)
fit3 <- lm(Age ~ Pclass + Sex + SibSp + Fare, data=train.woNA)
predFit3 <- predict(fit3, train.woNA[,c(2, 3, 5, 7)])

set.seed(123)
fit4 <- lm(Age ~ Pclass + Sex + SibSp, data=train.woNA)
predFit4 <- predict(fit4, train.woNA[,c(2, 3, 5)])

set.seed(123)
# Using step function
step(fit1, ~.^2)
set.seed(123)
fit5 <- lm(formula = Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + 
               Pclass:Parch + Parch:Fare + Sex:Parch + Sex:Embarked + Pclass:SibSp + 
               SibSp:Fare + Pclass:Embarked, data = train.woNA)
predFit5 <- predict(fit5, train.woNA)
anova(fit1, fit2, fit3, fit4, fit5)

par(mfrow=c(2,2))
plot(fit5)

set.seed(123)
fit6 <- train(Age ~., data=train.woNA[,-1], method="rf")
predFit6 <- predict(fit6, train.woNA)

df <- data.frame(predFit1, predFit2, predFit3, predFit4, predFit5, predFit6, 
                 train.woNA$Age)
#View(df)

# Selecting fit6 as our final model
age_predictions <- predict(fit6, train.na[,-4])
# sum(age_predictions < 0) # only fit6 has sum = 0
train$Age[na_Age] <- age_predictions
```

# Step 3: Analyze Plots
## Survivality
```{r echo = FALSE, message=FALSE}
library(ggplot2)
library(gridExtra)

# histogram for Survived column.
ggplot(data=train, aes(Survived)) + geom_histogram(binwidth=0.5) + 
    ggtitle("Fig.1: Histogram of Survivality")
```

We observe from this plot that almost 60% didn't survive the disaster.
## Survivality by Age and Sex
```{r echo = FALSE, message=FALSE}
# Jitter plot for Age, Sex and Survived columns.
ggplot(data=train, aes(x=Age, y=Sex, color=Survived)) + 
    geom_point(position="jitter") + 
    ggtitle("Fig.2: Comparision of Survivality by Age and Sex") + 
    xlab("Age (in years)")
```

We observe from this plot that higher percentage of females across all ages
survived whereas for males, there was higher survival rate for children but 
lower survival rate for adults.This confirms the "Women and Children First" 
policy used for the lifeboats.
## Survivality by no. of Siblings/Spouse and Age
```{r echo=FALSE}
ggplot(data=train, aes(x=SibSp, y=Age, color=Survived)) + 
    geom_point(position="jitter") + 
    ggtitle("Fig.3: Comparision of Survivality by Age and Sex") +
    xlab("No. of Siblings") + ylab("Age (in years)")
```

We observe from the above plot that as the no. of siblings/spouse increase for ages<20,
survivality has decreased.
We plot a histogram to confirm the above observation.
```{r echo=FALSE, message=FALSE}
library(scales)
ggplot(data=train, aes(x=SibSp, fill=Survived)) + 
    geom_bar(aes(y=(..count..)/sum(..count..))) + 
    scale_y_continuous(labels=percent) + 
    ggtitle("Fig.4: Percentage plot of no.of Siblings/Spouse and Survivality") + 
    xlab("Percentage of counts") + ylab("No. of Siblings")
```

The distribution of the number of siblings/spouse is not uniform to prove our theory.
we plot another histogram and this time we show the relative frequencies.
```{r echo=FALSE, message=FALSE}
ggplot(train, aes(x=SibSp, fill=Survived)) + geom_bar(position="fill") + 
    ggtitle("Fig.5: Relative Frequencies of no.of Siblings/Spouse and Survivality") + 
    xlab("No. of Siblings") 
```

The above plot confirms that as the no. of siblings/spouse increase for ages <20, the 
survivality chances decreases.
We also observe that the survival rate of people with zero siblings/spouse is around 10% 
lower than the survival rate of people with 1 sibling. 
From Fig.3, we can infer that this is because most people with zero siblings/spouse are
ages>20 who (from Fig.2) had lower survivality rate.

## Survivality by Age Group
```{r}
train$AgeGroup <- cut(train$Age, breaks = c(0, 8, 13, 18, 60, Inf), labels = c("Child", "Teenager", "Young Adult", "Adult", "Elder"), right=FALSE)
ggplot(data=train, aes(AgeGroup, fill=Survived)) + geom_histogram(binwidth=1)
```

# Step 4: Create Data Partitions
We divide the training set into 3 parts, in a 60:20:20 manner.
```{r message=FALSE}
# TODO: Use different splitting methods: k-fold, repeat k-fold, leave-one out.
library(caret)
train <- subset(train, select=-AgeGroup)
set.seed(123) # setting seed for reproducibility
# We first divide the training set into two parts in 60:40 ratio.
inTrain <- createDataPartition(y=train$Survived, p = 0.6, list=FALSE)
training.train <- train[inTrain,]
rest_train <- train[-inTrain,]

set.seed(123) # setting seed for reproducibility
# We now divide the rest of the training set into two parts in 50:50 ratio.
# This splits the original training set in a 60:20:20 ratio.
inTest <- createDataPartition(y=rest_train$Survived, p = 0.5, list=FALSE)
# 20% data for validation set to select best classifier
training.classifier_valid <- rest_train[inTest,] 
# 20% data for validation set to calculate out-of-sample errors
training.oos_valid <- rest_train[-inTest,] 

# data_labels <- c("Train", "Validate", "Test")
# per_data_point_label <- sample(data_labels, size=nrow(train), 
#                                replace=TRUE, prob=c(0.6, 0.2, 0.2))
# training.train <- train[per_data_point_label == "Train",]
# training.classifier_valid <- train[per_data_point_label == "Validate",]
# training.oos_valid <- train[per_data_point_label == "Test",]
```
# Step 5: Select the best classifier
We select among different classifiers (Decision trees, Random Forests, 
Naive Bayes, etc.) by training on training.train set and and predicting on 
training.classifier_valid.
```{r message=FALSE}
# TODO: Use interactions b/w variables
library(klaR)
par(mfrow=c(1,1))
# Decision Trees
set.seed(123)
tree_Model <- train(Survived ~ ., data=training.train, method="rpart")
pred_Tree <- predict(tree_Model, newdata=training.classifier_valid[,-1])
conf_Tree <- confusionMatrix(pred_Tree, training.classifier_valid$Survived)
# Decision Tree Plot
library(rattle)
fancyRpartPlot(tree_Model$finalModel)

# Random Forest
set.seed(123)
forest_Model <- train(Survived ~ ., data=training.train, method="rf")
pred_Forest <- predict(forest_Model, newdata=training.classifier_valid[,-1])
conf_Forest <- confusionMatrix(pred_Forest, training.classifier_valid$Survived)

# Naive Bayes
set.seed(123)
naiveB_Model <- NaiveBayes(Survived ~ ., data=training.train)
pred_naiveB <- predict(naiveB_Model, newdata=training.classifier_valid[,-1])
conf_naiveB <- confusionMatrix(pred_naiveB$class, training.classifier_valid$Survived)

#pred1 <- predict(mod1, testing)
#pred2 <- predict(logit_Model, testing)
#qplot(pred1, pred2, color=wage, data=testing)

# Ensemble: Fit a model that combines predictors
df_Ensemble <- data.frame(pTree=pred_Tree, pForest=pred_Forest, 
                          pNaive=pred_naiveB$class, 
                          Survived=training.classifier_valid$Survived)
set.seed(123)
ensemble_Model <- train(Survived ~., method="gbm", data=df_Ensemble)
pred_Ensemble <- predict(ensemble_Model, df_Ensemble[,-4])
conf_Ensemble <- confusionMatrix(pred_Ensemble, df_Ensemble$Survived)

# Boost
set.seed(123)
boost_Model <- train(Survived ~., method="gbm", data=training.train)
pred_Boost <- predict(boost_Model, newdata=training.classifier_valid[,-1])
conf_Boost <- confusionMatrix(pred_Boost, training.classifier_valid$Survived)
```
Now that all the predictions are complete, we compare them
```{r}
# Comparison of predictions from each model
compare_df <- data.frame(Accuracy = c(conf_Tree$overall[1], 
                                      conf_Forest$overall[1], 
                                      conf_naiveB$overall[1],
                                      conf_Ensemble$overall[1],
                                      conf_Boost$overall[1]),
                         row.names = c("rpart", "rf", "NaiveB", "Ensemble", "Boost"))
compare_df
```
We observe that both **Random Forest** and **Boost** gives us the highest Accuracy,
we see how the above model fares with the training.oos_valid dataset
```{r}
pred_Tree_oos <- predict(tree_Model, newdata=training.oos_valid[,-1])
conf_Tree_oos <- confusionMatrix(pred_Tree_oos, training.oos_valid$Survived)

pred_Forest_oos <- predict(forest_Model, newdata=training.oos_valid[,-1])
conf_Forest_oos <- confusionMatrix(pred_Forest_oos, training.oos_valid$Survived)

pred_naiveB_oos <- predict(naiveB_Model, newdata=training.oos_valid[,-1])
conf_naiveB_oos <- confusionMatrix(pred_naiveB_oos$class, training.oos_valid$Survived)

df_ensemble_oos <- data.frame(pTree=pred_Tree_oos, pForest=pred_Forest_oos, 
                              pNaive=pred_naiveB_oos$class, 
                              Survived=training.oos_valid$Survived)
pred_Ensemble_oos <- predict(ensemble_Model, df_ensemble_oos[,-4])
conf_Ensemble_oos <- confusionMatrix(pred_Ensemble_oos, df_ensemble_oos$Survived)

pred_Boost_oos <- predict(boost_Model, newdata=training.oos_valid[,-1])
conf_Boost_oos <- confusionMatrix(pred_Boost_oos, training.oos_valid$Survived)

compare_df_oos <- data.frame(Accuracy = c(conf_Tree_oos$overall[1], 
                                          conf_Forest_oos$overall[1], 
                                          conf_naiveB_oos$overall[1],
                                          conf_Ensemble_oos$overall[1],
                                          conf_Boost_oos$overall[1]),
                             row.names = c("rpart", "rf", "NaiveB", "Ensemble", "Boost"))
compare_df_oos
```
We observe that **Random Forest** gives us the highest out-of-sample accuracy.
We select this classifier for predicting the test set.
# Step 6: Predict on Test set
## Step 6.1: Clean Test set and syncc columns with training set
```{r}
# remove unnecesary columns
test <- subset(inp_test, select = -c(Name, Ticket, Cabin))
test$Pclass <- factor(test$Pclass)
test$Sex <- factor(test$Sex)
test$Embarked <- factor(test$Embarked)
test$Fare[which(is.na(test$Fare))] <- 0
na_Age <- which(is.na(test$Age))
test.na <- test[na_Age,]
# Predicting the age in the test set using the fit6 model
age_predictions <- predict(fit6, test.na[,-c(1, 4)])
# sum(age_predictions < 0) # only fit6 has sum = 0
test$Age[na_Age] <- age_predictions
```
## Step 6.2: predict on the classifier selected above
```{r}
# we train the selected classifier using the full training set and predict on
# the test set
set.seed(123)
forest_Model <- train(Survived ~ ., data=train, method="rf")
pred_Forest_test <- predict(forest_Model, newdata=test)

# converting factor to numeric(Not_Survived=0, Survived=1)
final_pred <- as.numeric(pred_Forest_test=="Survived")
df <- data.frame(PassengerId=test$PassengerId, Survived=final_pred)    
```
# Step 7: Write results to csv and upload to Kaggle
```{r}
write.csv(df, file="prediction_rf.csv", row.names=FALSE)
```
Accuracy: 0.77033
We find that the above model results in an accuracy of 0.77033 in Kaggle.
That's Awesome! and far better than our earlier score of 0.73.
So, predicting the missing Ages paid off :)
Again, just for fun, let's predict on the test set using some of the other 
classfiers.
```{r}
# Decision Tree
set.seed(123)
tree_Model <- train(Survived ~ ., data=train, method="rpart")
pred_Tree_test <- predict(tree_Model, newdata=test)
fancyRpartPlot(tree_Model$finalModel)

# converting factor to numeric(Not_Survived=0, Survived=1)
final_pred <- as.numeric(pred_Tree_test=="Survived")
df <- data.frame(PassengerId=test$PassengerId, Survived=final_pred)

write.csv(df, file="prediction_tree.csv", row.names=FALSE)
```
Decision Tree scored 0.78469, better than the RF model.Yay!
But that's a little bit unexpected since the Rf model did better(not by much) 
than the Decision Tree model.

```{r}
# Naive Bayes
set.seed(123)
naiveB_Model <- NaiveBayes(Survived ~ ., data=train)
pred_naiveB_test <- predict(naiveB_Model, newdata=test)

# converting factor to numeric(Not_Survived=0, Survived=1)
final_pred <- as.numeric(pred_naiveB_test$class=="Survived")
df <- data.frame(PassengerId=test$PassengerId, Survived=final_pred)

write.csv(df, file="prediction_naiveB.csv", row.names=FALSE)
```
Naive Bayes scored 0.75120

```{r}
# Ensemble
df_Ensemble_test <- data.frame(pTree=pred_Tree_test, pForest=pred_Forest_test, 
                               pNaive=pred_naiveB_test$class)
pred_Ensemble_test <- predict(ensemble_Model, newdata=df_Ensemble_test)

# converting factor to numeric(Not_Survived=0, Survived=1)
final_pred <- as.numeric(pred_Ensemble_test=="Survived")
df <- data.frame(PassengerId=test$PassengerId, Survived=final_pred)

write.csv(df, file="prediction_ensemble.csv", row.names=FALSE)
```
Ensemble scored 0.77033, which is exactly the score Rf gave us, not surprising 
since we used Rf as one of the models to create this ensemble one.

```{r}
# Generalized Boosted Regression Models
set.seed(123)
boost_Model <- train(Survived ~., method="gbm", data=train)
pred_Boost_test <- predict(boost_Model, newdata=test)

# converting factor to numeric(Not_Survived=0, Survived=1)
final_pred <- as.numeric(pred_Boost_test=="Survived")
df <- data.frame(PassengerId=test$PassengerId, Survived=final_pred)

write.csv(df, file="prediction_boost.csv", row.names=FALSE)
```
Boost scored 0.76077

We observe from the above values that our Decision Tree model was the best model.
But I believe it can be made better.

Next, we decide on ways to use the Name, Ticket and Cabin columns and see if we 
can improve the score.